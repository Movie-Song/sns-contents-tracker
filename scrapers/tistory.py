import feedparser
from datetime import datetime
from typing import List, Dict
from urllib.parse import urlparse

class TistoryScraper:
    """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì˜ RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, blog_url: str, blog_name: str = None):
        """
        Args:
            blog_url: í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ URL (ì˜ˆ: https://yourblog.tistory.com)
            blog_name: ë¸”ë¡œê·¸ ì´ë¦„ (ê¸°ë³¸ê°’: URLì—ì„œ ìë™ ì¶”ì¶œ)
        """
        self.blog_url = blog_url.rstrip('/')
        # í‹°ìŠ¤í† ë¦¬ RSS í”¼ë“œ URL
        self.rss_url = f"{self.blog_url}/rss"
        
        # ë¸”ë¡œê·¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ
        if blog_name is None:
            blog_name = self._extract_blog_name(blog_url)
        
        self.platform = f"Tistory ({blog_name})"
    
    def _extract_blog_name(self, url: str) -> str:
        """
        URLì—ì„œ ë¸”ë¡œê·¸ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            url: ë¸”ë¡œê·¸ URL
        
        Returns:
            str: ë¸”ë¡œê·¸ ì´ë¦„
        """
        try:
            # https://pro-editor.tistory.com -> pro-editor
            # https://custom-domain.com -> custom-domain
            parsed = urlparse(url)
            hostname = parsed.hostname or ''
            # tistory.com ì œê±°
            blog_name = hostname.replace('.tistory.com', '').replace('www.', '')
            return blog_name if blog_name else 'tistory'
        except:
            return 'tistory'
    
    def fetch_posts(self, limit: int = 50) -> List[Dict]:
        """
        RSS í”¼ë“œì—ì„œ ìµœì‹  í¬ìŠ¤íŠ¸ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            limit: ê°€ì ¸ì˜¬ ìµœëŒ€ í¬ìŠ¤íŠ¸ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
        
        Returns:
            List[Dict]: í¬ìŠ¤íŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                - title: ì œëª©
                - url: URL
                - published_date: ë°œí–‰ì¼ (YYYY-MM-DD í˜•ì‹)
                - platform: í”Œë«í¼ ì´ë¦„
        """
        print(f"ğŸ” {self.platform} í”¼ë“œ í™•ì¸ ì¤‘: {self.rss_url}")
        
        try:
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(self.rss_url)
            
            if feed.bozo:  # íŒŒì‹± ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš°
                print(f"âš ï¸  RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {feed.bozo_exception}")
                return []
            
            if not feed.entries:
                print(f"âš ï¸  í”¼ë“œì— í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            posts = []
            for entry in feed.entries[:limit]:
                # ì œëª©
                title = entry.get('title', 'ì œëª© ì—†ìŒ')
                
                # URL
                url = entry.get('link', '')
                
                # ë°œí–‰ì¼ íŒŒì‹±
                published_date = self._parse_date(entry)
                
                if url and published_date:
                    posts.append({
                        'title': title,
                        'url': url,
                        'published_date': published_date,
                        'platform': self.platform
                    })
            
            print(f"âœ… {len(posts)}ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return posts
            
        except Exception as e:
            print(f"âŒ RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _parse_date(self, entry) -> str:
        """
        RSS ì—”íŠ¸ë¦¬ì—ì„œ ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            entry: feedparser entry ê°ì²´
        
        Returns:
            str: YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´
        """
        # published_parsed ë˜ëŠ” updated_parsed ì‚¬ìš©
        date_tuple = entry.get('published_parsed') or entry.get('updated_parsed')
        
        if date_tuple:
            try:
                dt = datetime(*date_tuple[:6])
                return dt.strftime("%Y-%m-%dT%H:%M:%S")
            except:
                pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return None
    
    def get_recent_posts(self, days: int = 30) -> List[Dict]:
        """
        ìµœê·¼ Nì¼ ì´ë‚´ì˜ í¬ìŠ¤íŠ¸ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 30ì¼)
        
        Returns:
            List[Dict]: ìµœê·¼ í¬ìŠ¤íŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        from datetime import timedelta
        
        all_posts = self.fetch_posts()
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        
        recent_posts = [
            post for post in all_posts 
            if post['published_date'] >= cutoff_date
        ]
        
        print(f"ğŸ“… ìµœê·¼ {days}ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸: {len(recent_posts)}ê°œ")
        return recent_posts


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import os
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ URL ê°€ì ¸ì˜¤ê¸°
    test_blog_url = os.getenv('TISTORY_BLOGS', 'https://pro-editor.tistory.com')
    
    print("=== í‹°ìŠ¤í† ë¦¬ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ===\n")
    
    scraper = TistoryScraper(test_blog_url)
    
    # ìµœê·¼ 10ê°œ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    posts = scraper.fetch_posts(limit=10)
    
    if posts:
        print(f"\nğŸ“ ìµœê·¼ í¬ìŠ¤íŠ¸ ëª©ë¡:")
        for i, post in enumerate(posts, 1):
            print(f"{i}. {post['title']}")
            print(f"   URL: {post['url']}")
            print(f"   ë‚ ì§œ: {post['published_date']}\n")
    else:
        print("\nâš ï¸  í¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
